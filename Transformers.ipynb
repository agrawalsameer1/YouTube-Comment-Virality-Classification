{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"gLKh4TeOYT_r","executionInfo":{"status":"ok","timestamp":1765166914173,"user_tz":480,"elapsed":2,"user":{"displayName":"Sameer Agrawal","userId":"18062353540738892167"}}},"outputs":[],"source":["import datetime as dt"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"-B0hAFGaW4S2","executionInfo":{"status":"ok","timestamp":1765166917070,"user_tz":480,"elapsed":3,"user":{"displayName":"Sameer Agrawal","userId":"18062353540738892167"}}},"outputs":[],"source":["video_post_times = {\n","    \"7BCojznmtRE.csv\": dt.datetime(2020, 9, 5, 6, 45, 11, tzinfo=dt.timezone.utc),\n","    \"cIY95KCnnNk.csv\": dt.datetime(2022, 2, 26, 22, 0, 14, tzinfo=dt.timezone.utc),\n","    \"CrtuA5HWFoU.csv\": dt.datetime(2020, 10, 20, 10, 30, 11, tzinfo=dt.timezone.utc),\n","    \"hDkuUZ3F1GU.csv\": dt.datetime(2020, 11, 26, 23, 45, 10, tzinfo=dt.timezone.utc),\n","    \"hgdSJdeGF_0.csv\": dt.datetime(2020, 6, 19, 5, 35, 49, tzinfo=dt.timezone.utc),\n","    \"qqOxkuO3ip0.csv\": dt.datetime(2021, 7, 8, 22, 45, 11, tzinfo=dt.timezone.utc),\n","    \"RJ0jdO5ZfU4.csv\": dt.datetime(2021, 4, 23, 19, 00, 12, tzinfo=dt.timezone.utc),\n","    \"RTXS4MMngnA.csv\": dt.datetime(2020, 5, 29, 4, 38, 30, tzinfo=dt.timezone.utc),\n","    \"tylNqtyj0gs.csv\": dt.datetime(2020, 8, 7, 7, 45, 12, tzinfo=dt.timezone.utc),\n","    \"Y7t5B69G0Dw.csv\": dt.datetime(2020, 7, 20, 11, 40, 34, tzinfo=dt.timezone.utc)\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PH7RlIoVZpWR"},"outputs":[],"source":["from pathlib import Path\n","import pandas as pd\n","import glob\n","import datetime as dt\n","\n","for filepath in glob.glob(\"*.csv\"):\n","  if 'annotated' in filepath:\n","    continue\n","\n","  base_dt = video_post_times[filepath]\n","\n","  df = pd.read_csv(filepath)\n","  if 'timestamp' not in df.columns:\n","      raise ValueError(\"CSV must have a 'timestamp' column (seconds).\")\n","\n","  df['timestamp'] = pd.to_numeric(df['timestamp'], errors='coerce')\n","  if df['timestamp'].isna().any():\n","      raise ValueError(\"Found non-numeric timestamps; clean or convert them first.\")\n","\n","  df['event_dt'] = pd.to_datetime(base_dt) + pd.to_timedelta(df['timestamp'], unit='s')\n","\n","  # Get weekday and hour (0–23) in UTC\n","  df['day_of_week_utc'] = df['event_dt'].dt.weekday\n","  df['hour_utc'] = df['event_dt'].dt.hour\n","\n","  # Optionally drop intermediate column\n","  df = df.drop(columns=['event_dt'])\n","\n","  output_path = Path(filepath).with_name(Path(filepath).stem + \"_annotated.csv\")\n","  df.to_csv(output_path, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1500,"status":"ok","timestamp":1764820428193,"user":{"displayName":"Sameer Agrawal","userId":"18062353540738892167"},"user_tz":480},"id":"6vTZqLR9esBs","outputId":"28bd03ac-f66d-4e4e-f24f-909b63130272"},"outputs":[{"name":"stdout","output_type":"stream","text":["Saved preprocessed data to dataset_processed.csv\n","Saved scaler to scaler.pkl\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","import pickle\n","\n","class Config:\n","    raw_csv = \"dataset.csv\"\n","    processed_csv = \"dataset_processed.csv\"\n","    scaler_path = \"scaler.pkl\"\n","\n","    text_col = \"comment\"\n","    timestamp_col = \"timestamp\"\n","    day_col = \"day\"\n","    hour_col = \"hour\"\n","    label_col = \"viral\"\n","\n","cfg = Config()\n","\n","def preprocess_delta(series: pd.Series) -> np.ndarray:\n","    return np.sign(series.astype(float).values) * np.log1p(np.abs(series.astype(float).values))\n","\n","def encode_day_of_week(series: pd.Series) -> np.ndarray:\n","    d = series.astype(float).values\n","    sin = np.sin(2 * np.pi * d / 7)\n","    cos = np.cos(2 * np.pi * d / 7)\n","    return np.stack([sin, cos], axis=1)\n","\n","def encode_hour_of_day(series: pd.Series) -> np.ndarray:\n","    h = series.astype(float).values\n","    sin = np.sin(2 * np.pi * h / 24)\n","    cos = np.cos(2 * np.pi * h / 24)\n","    return np.stack([sin, cos], axis=1)\n","\n","# Load raw data\n","df = pd.read_csv(cfg.raw_csv)\n","df = df.dropna(subset=[cfg.text_col, cfg.timestamp_col, cfg.day_col, cfg.hour_col, cfg.label_col])\n","\n","# Preprocess timestamp\n","delta_raw = preprocess_delta(df[cfg.timestamp_col])\n","scaler = StandardScaler().fit(delta_raw.reshape(-1, 1))\n","df['timestamp_scaled'] = scaler.transform(delta_raw.reshape(-1, 1)).reshape(-1)\n","\n","# Cyclic encodings\n","day_encoded = encode_day_of_week(df[cfg.day_col])\n","df['day_sin'] = day_encoded[:, 0]\n","df['day_cos'] = day_encoded[:, 1]\n","\n","hour_encoded = encode_hour_of_day(df[cfg.hour_col])\n","df['hour_sin'] = hour_encoded[:, 0]\n","df['hour_cos'] = hour_encoded[:, 1]\n","\n","df = df[['comment',\n","         'timestamp_scaled', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos',\n","         'viral']]\n","\n","# Save processed dataset\n","df.to_csv(cfg.processed_csv, index=False)\n","print(f\"Saved preprocessed data to {cfg.processed_csv}\")\n","print(f\"Saved scaler to {cfg.scaler_path}\")"]},{"cell_type":"code","source":["pip install --upgrade transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":638},"id":"QbgHscj-Jozv","executionInfo":{"status":"ok","timestamp":1765166993093,"user_tz":480,"elapsed":14019,"user":{"displayName":"Sameer Agrawal","userId":"18062353540738892167"}},"outputId":"c0d75393-b335-4f21-83b0-abe1de0502bf"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n","Collecting transformers\n","  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n","Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: transformers\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.57.2\n","    Uninstalling transformers-4.57.2:\n","      Successfully uninstalled transformers-4.57.2\n","Successfully installed transformers-4.57.3\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["transformers"]},"id":"015fb3ee16a64508be9c166c8ea500c0"}},"metadata":{}}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2412131,"status":"ok","timestamp":1765173316043,"user":{"displayName":"Sameer Agrawal","userId":"18062353540738892167"},"user_tz":480},"id":"z5ax7_MVBKgN","outputId":"d8e79e42-5c2e-4e07-f86f-2f130deb9f85"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Train: 90598, Val: 19415, Test: 19415\n","Epoch 1/4 | Train Loss: 0.5688 | Val Acc: 0.8970 | Val F1: 0.7373 | Val ROC-AUC: 0.9539\n","Saved best model to best_viral_classifier.pt\n","Epoch 2/4 | Train Loss: 0.4155 | Val Acc: 0.9001 | Val F1: 0.7453 | Val ROC-AUC: 0.9566\n","Saved best model to best_viral_classifier.pt\n","Epoch 3/4 | Train Loss: 0.3369 | Val Acc: 0.9094 | Val F1: 0.7565 | Val ROC-AUC: 0.9558\n","Saved best model to best_viral_classifier.pt\n","Epoch 4/4 | Train Loss: 0.2742 | Val Acc: 0.9047 | Val F1: 0.7475 | Val ROC-AUC: 0.9539\n","Test Acc: 0.9059 | Test F1: 0.7485 | Test ROC-AUC: 0.9555\n"]}],"source":["# BASE TRANSFORMER\n","\n","import os\n","import math\n","import random\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n","from transformers import (\n","    DistilBertModel,\n","    DistilBertTokenizerFast,\n","    DataCollatorWithPadding,\n",")\n","from transformers.optimization import get_linear_schedule_with_warmup\n","from torch.optim import AdamW\n","import json\n","\n","class Config:\n","    data_csv = \"dataset_processed.csv\"  # preprocessed CSV with all features\n","    text_col = \"comment\"\n","\n","    # Preprocessed numeric feature columns\n","    numeric_features = ['timestamp_scaled', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos']\n","    label_col = \"viral\"\n","\n","    model_name = \"distilbert-base-uncased\"\n","    max_length = 256\n","    batch_size = 32\n","    epochs = 4\n","    lr = 2e-5\n","    weight_decay = 0.01\n","    warmup_ratio = 0.1\n","    dropout = 0.2\n","    delta_proj_dim = 64\n","    seed = 42\n","    freeze_transformer = False\n","    save_best_path = \"best_viral_classifier.pt\"\n","\n","\n","cfg = Config()\n","\n","def set_seed(seed: int):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","set_seed(cfg.seed)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","TIMESTAMP_MEAN = 11.800730451380801\n","TIMESTAMP_STD = 4.351129933194433\n","\n","df = pd.read_csv(cfg.data_csv)\n","\n","# Basic cleaning/checks\n","required_cols = [cfg.text_col] + cfg.numeric_features + [cfg.label_col]\n","missing = [c for c in required_cols if c not in df.columns]\n","if missing:\n","    raise ValueError(f\"Missing columns in CSV: {missing}\")\n","\n","df = df.dropna(subset=required_cols)\n","df[cfg.label_col] = df[cfg.label_col].astype(int)\n","\n","train_df, test_df = train_test_split(df, test_size=0.15, random_state=cfg.seed, stratify=df[cfg.label_col])\n","train_df, val_df = train_test_split(train_df, test_size=0.15 * (1/0.85), random_state=cfg.seed, stratify=train_df[cfg.label_col])\n","\n","train_numeric = train_df[cfg.numeric_features].values\n","val_numeric = val_df[cfg.numeric_features].values\n","test_numeric = test_df[cfg.numeric_features].values\n","\n","print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n","\n","tokenizer = DistilBertTokenizerFast.from_pretrained(cfg.model_name)\n","\n","class CommentDataset(Dataset):\n","    def __init__(self, texts, numeric_feats, labels, tokenizer, max_length):\n","        self.texts = list(texts)\n","        self.numeric = torch.tensor(numeric_feats, dtype=torch.float32)\n","        self.labels = torch.tensor(np.asarray(labels, dtype=np.float32))\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = str(self.texts[idx])\n","        enc = self.tokenizer(\n","            text,\n","            truncation=True,\n","            max_length=self.max_length,\n","            padding=False,\n","            return_tensors=None,\n","        )\n","        return {\n","            \"input_ids\": torch.tensor(enc[\"input_ids\"], dtype=torch.long).squeeze(0),\n","            \"attention_mask\": torch.tensor(enc[\"attention_mask\"], dtype=torch.long).squeeze(0),\n","            \"numeric\": self.numeric[idx],  # shape (5,)\n","            \"labels\": self.labels[idx],\n","        }\n","\n","train_ds = CommentDataset(train_df[cfg.text_col], train_numeric, train_df[cfg.label_col], tokenizer, cfg.max_length)\n","val_ds   = CommentDataset(val_df[cfg.text_col],   val_numeric,   val_df[cfg.label_col],   tokenizer, cfg.max_length)\n","test_ds  = CommentDataset(test_df[cfg.text_col],  test_numeric,  test_df[cfg.label_col],  tokenizer, cfg.max_length)\n","\n","base_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","def collate_fn(batch):\n","    token_batch = [{\"input_ids\": b[\"input_ids\"], \"attention_mask\": b[\"attention_mask\"]} for b in batch]\n","    collated = base_collator(token_batch)\n","    numerics = torch.stack([b[\"numeric\"] for b in batch])  # (B,5)\n","    labels = torch.stack([b[\"labels\"] for b in batch])\n","    collated[\"numeric\"] = numerics\n","    collated[\"labels\"] = labels\n","    return collated\n","\n","train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, collate_fn=collate_fn)\n","val_loader   = DataLoader(val_ds,   batch_size=cfg.batch_size, shuffle=False, collate_fn=collate_fn)\n","test_loader  = DataLoader(test_ds,  batch_size=cfg.batch_size, shuffle=False, collate_fn=collate_fn)\n","\n","class ViralCommentClassifier(nn.Module):\n","    def __init__(self, model_name, numeric_in_dim=5, delta_proj_dim=64, dropout=0.2, freeze_transformer=False):\n","        super().__init__()\n","        self.transformer = DistilBertModel.from_pretrained(model_name)\n","        if freeze_transformer:\n","            for p in self.transformer.parameters():\n","                p.requires_grad = False\n","\n","        hidden_size = self.transformer.config.hidden_size  # 768\n","\n","        # Project numeric vector (delta + cyc encodings) to feature vector\n","        self.numeric_proj = nn.Sequential(\n","            nn.Linear(numeric_in_dim, delta_proj_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(delta_proj_dim, delta_proj_dim),\n","            nn.GELU(),\n","        )\n","\n","        combined_dim = hidden_size + delta_proj_dim\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(dropout),\n","            nn.Linear(combined_dim, 256),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(256, 1),\n","        )\n","\n","    def forward(self, input_ids, attention_mask, numeric):\n","        out = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n","        cls_emb = out.last_hidden_state[:, 0, :]\n","        num_emb = self.numeric_proj(numeric)\n","        x = torch.cat([cls_emb, num_emb], dim=-1)\n","        logits = self.classifier(x).squeeze(-1)\n","        return logits\n","\n","model = ViralCommentClassifier(\n","    cfg.model_name,\n","    numeric_in_dim=len(cfg.numeric_features),  # 5\n","    delta_proj_dim=cfg.delta_proj_dim,\n","    dropout=cfg.dropout,\n","    freeze_transformer=cfg.freeze_transformer\n",").to(device)\n","\n","train_labels_np = train_df[cfg.label_col].values\n","pos_count = (train_labels_np == 1).sum()\n","neg_count = (train_labels_np == 0).sum()\n","pos_weight = torch.tensor(neg_count / max(pos_count, 1), dtype=torch.float32).to(device)\n","criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n","\n","optimizer = AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n","num_training_steps = len(train_loader) * cfg.epochs\n","num_warmup_steps = int(cfg.warmup_ratio * num_training_steps)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n","\n","def evaluate(loader, model):\n","    model.eval()\n","    all_labels = []\n","    all_probs = []\n","    with torch.no_grad():\n","        for batch in loader:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            numeric = batch[\"numeric\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","\n","            logits = model(input_ids, attention_mask, numeric)\n","            probs = torch.sigmoid(logits)\n","            all_labels.append(labels.cpu().numpy())\n","            all_probs.append(probs.cpu().numpy())\n","\n","    all_labels = np.concatenate(all_labels)\n","    all_probs = np.concatenate(all_probs)\n","    preds = (all_probs >= 0.5).astype(np.int32)\n","\n","    acc = accuracy_score(all_labels, preds)\n","    f1 = f1_score(all_labels, preds)\n","    try:\n","        roc = roc_auc_score(all_labels, all_probs)\n","    except ValueError:\n","        roc = float(\"nan\")\n","    return acc, f1, roc\n","\n","#Training\n","best_val_f1 = -1.0\n","for epoch in range(1, cfg.epochs + 1):\n","    model.train()\n","    total_loss = 0.0\n","    for batch in train_loader:\n","        optimizer.zero_grad()\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        numeric = batch[\"numeric\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","\n","        logits = model(input_ids, attention_mask, numeric)\n","        loss = criterion(logits, labels)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        scheduler.step()\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(train_loader)\n","    val_acc, val_f1, val_roc = evaluate(val_loader, model)\n","    print(f\"Epoch {epoch}/{cfg.epochs} | Train Loss: {avg_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f} | Val ROC-AUC: {val_roc:.4f}\")\n","\n","    if val_f1 > best_val_f1:\n","        best_val_f1 = val_f1\n","        torch.save({\n","            \"model_state_dict\": model.state_dict(),\n","            \"timestamp_mean\": TIMESTAMP_MEAN,\n","            \"timestamp_std\": TIMESTAMP_STD,\n","            \"tokenizer_name\": cfg.model_name,\n","            \"config\": vars(cfg),\n","        }, cfg.save_best_path)\n","        print(f\"Saved best model to {cfg.save_best_path}\")\n","\n","test_acc, test_f1, test_roc = evaluate(test_loader, model)\n","print(f\"Test Acc: {test_acc:.4f} | Test F1: {test_f1:.4f} | Test ROC-AUC: {test_roc:.4f}\")\n","\n","def preprocess_numeric_for_inference(timestamp_seconds, day, hour, timestamp_mean, timestamp_std):\n","    # delta: log1p then standardize\n","    delta = math.log1p(float(timestamp_seconds))\n","    delta = (delta - timestamp_mean) / timestamp_std\n","    # cyc encodings\n","    day = float(day)\n","    hour = float(hour)\n","    day_sin = math.sin(2 * math.pi * day / 7)\n","    day_cos = math.cos(2 * math.pi * day / 7)\n","    hour_sin = math.sin(2 * math.pi * hour / 24)\n","    hour_cos = math.cos(2 * math.pi * hour / 24)\n","    return np.array([delta, day_sin, day_cos, hour_sin, hour_cos], dtype=np.float32)\n","\n","\n","def predict_viral(texts, timestamps, days, hours, model, tokenizer, timestamp_mean, timestamp_std, max_length=cfg.max_length, batch_size=32):\n","    model.eval()\n","    batches = []\n","    for i in range(0, len(texts), batch_size):\n","        batch_texts = texts[i:i+batch_size]\n","        batch_ts = timestamps[i:i+batch_size]\n","        batch_days = days[i:i+batch_size]\n","        batch_hours = hours[i:i+batch_size]\n","        enc = tokenizer(batch_texts, truncation=True, max_length=max_length, padding=True, return_tensors=\"pt\")\n","        numerics = [preprocess_numeric_for_inference(t, d, h, timestamp_mean, timestamp_std) for t, d, h in zip(batch_ts, batch_days, batch_hours)]\n","        numerics = torch.tensor(numerics, dtype=torch.float32)\n","        batches.append((enc, numerics))\n","\n","    all_probs = []\n","    with torch.no_grad():\n","        for enc, numerics in batches:\n","            input_ids = enc[\"input_ids\"].to(device)\n","            attention_mask = enc[\"attention_mask\"].to(device)\n","            numerics = numerics.to(device)\n","            logits = model(input_ids, attention_mask, numerics)\n","            probs = torch.sigmoid(logits).cpu().numpy()\n","            all_probs.extend(probs.tolist())\n","\n","    preds = [1 if p >= 0.5 else 0 for p in all_probs]\n","    return preds, all_probs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"sMWaaVCdukNL","outputId":"22ca6525-8fed-47d4-fa5b-b8fffab9afe2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n","Dataset size: 129428\n","Viral comments: 21572 (16.67%)\n","Train: 90595, Val: 19418, Test: 19415\n","Splitting comments into sentences...\n","Sentences per comment - Mean: 1.44, Median: 1, Max: 10\n","Numeric features shape: (90595, 5)\n","Numeric features (first row): [-0.3422516  0.         1.        -0.8660254  0.5      ]\n","\n","================================================================================\n","MODEL PARAMETER COUNT\n","================================================================================\n","Total parameters:      66,679,490\n","Trainable parameters:  66,679,490\n","Frozen parameters:     0\n","Trainable percentage:  100.00%\n","================================================================================\n","\n","Parameter breakdown by component:\n","--------------------------------------------------------------------------------\n","transformer                    Total:   66,362,880  Trainable:   66,362,880\n","sentence_attention             Total:       98,561  Trainable:       98,561\n","numeric_proj                   Total:        4,544  Trainable:        4,544\n","classifier                     Total:      213,505  Trainable:      213,505\n","================================================================================\n","\n","Epoch 1/4\n","  Train Loss: 0.6495 | Train F1: 0.7655\n","  Val Loss: 0.6632 | Val F1: 0.7233 | Val Acc: 0.9010 | Val ROC-AUC: 0.9436 | Val PR-AUC: 0.7607\n","  ✓ Saved best model to best_hierarchical_viral_classifier.pt\n","\n","Epoch 2/4\n","  Train Loss: 0.5160 | Train F1: 0.8207\n","  Val Loss: 0.6451 | Val F1: 0.7278 | Val Acc: 0.9005 | Val ROC-AUC: 0.9482 | Val PR-AUC: 0.7829\n","  ✓ Saved best model to best_hierarchical_viral_classifier.pt\n","\n","Epoch 3/4\n","  Train Loss: 0.4465 | Train F1: 0.8684\n","  Val Loss: 0.7518 | Val F1: 0.7288 | Val Acc: 0.9013 | Val ROC-AUC: 0.9469 | Val PR-AUC: 0.7760\n","  ✓ Saved best model to best_hierarchical_viral_classifier.pt\n","\n","Epoch 4/4\n","  Train Loss: 0.3792 | Train F1: 0.8898\n","  Val Loss: 0.9256 | Val F1: 0.7299 | Val Acc: 0.9017 | Val ROC-AUC: 0.9446 | Val PR-AUC: 0.7649\n","  ✓ Saved best model to best_hierarchical_viral_classifier.pt\n","\n","\n","============================================================\n","FINAL TEST RESULTS:\n","============================================================\n","Test Loss: 0.8879 | Test Acc: 0.9065 | Test F1: 0.7429 | Test ROC-AUC: 0.9465 | Test PR-AUC: 0.7744\n","============================================================\n","\n","\n","================================================================================\n","EXAMPLE INFERENCE\n","================================================================================\n"]},{"ename":"TypeError","evalue":"'NoneType' object is not subscriptable","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-839603133.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0mexample_hour_cos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.87\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m     visualize_attention(\n\u001b[0m\u001b[1;32m    697\u001b[0m         \u001b[0mexample_comment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0mexample_timestamp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-839603133.py\u001b[0m in \u001b[0;36mvisualize_attention\u001b[0;34m(comment_text, timestamp_scaled, day_sin, day_cos, hour_sin, hour_cos, model, tokenizer)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[0;31m# Get attention weights (stored in model during forward pass)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0msentence_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_sentence_attention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m     \u001b[0mcontext_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_context_attention\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n{'='*80}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"]}],"source":["# HIERARCHICAL MODEL\n","\n","import os\n","import math\n","import random\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score\n","from transformers import (\n","    DistilBertModel,\n","    DistilBertTokenizerFast,\n","    DataCollatorWithPadding,\n",")\n","from transformers.optimization import get_linear_schedule_with_warmup\n","from torch.optim import AdamW\n","import re\n","\n","class Config:\n","    data_csv = \"dataset_processed.csv\"  # path to your preprocessed CSV\n","    text_col = \"comment\"\n","    timestamp_col = \"timestamp_scaled\"\n","    day_sin_col = \"day_sin\"\n","    day_cos_col = \"day_cos\"\n","    hour_sin_col = \"hour_sin\"\n","    hour_cos_col = \"hour_cos\"\n","    label_col = \"viral\"\n","\n","    model_name = \"distilbert-base-uncased\"\n","    max_length = 128\n","    batch_size = 16\n","    epochs = 4\n","    lr = 2e-5\n","    weight_decay = 0.01\n","    warmup_ratio = 0.1\n","    dropout = 0.2\n","    delta_proj_dim = 64\n","    seed = 42\n","    freeze_transformer = False\n","    save_best_path = \"best_hierarchical_viral_classifier.pt\"\n","\n","\n","cfg = Config()\n","\n","\n","def set_seed(seed: int):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","set_seed(cfg.seed)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","def split_into_sentences(text):\n","    \"\"\"Split comment into sentences. Handle YouTube comment quirks.\"\"\"\n","    text = str(text)\n","\n","    # Split on sentence-ending punctuation\n","    sentences = re.split(r'[.!?]+', text)\n","    sentences = [s.strip() for s in sentences if s.strip()]\n","\n","    # If no punctuation, try splitting on newlines\n","    if len(sentences) <= 1:\n","        sentences = [s.strip() for s in text.split('\\n') if s.strip()]\n","\n","    # If still nothing, just use the whole text\n","    if not sentences:\n","        sentences = [text.strip()]\n","\n","    # Cap at 10 sentences to avoid memory issues\n","    return sentences[:10]\n","\n","df = pd.read_csv(cfg.data_csv)\n","\n","# Basic cleaning/checks\n","required_cols = [\n","    cfg.text_col,\n","    cfg.timestamp_col,\n","    cfg.day_sin_col,\n","    cfg.day_cos_col,\n","    cfg.hour_sin_col,\n","    cfg.hour_cos_col,\n","    cfg.label_col\n","]\n","missing = [c for c in required_cols if c not in df.columns]\n","if missing:\n","    raise ValueError(f\"Missing columns in CSV: {missing}\")\n","\n","df = df.dropna(subset=required_cols)\n","df[cfg.label_col] = df[cfg.label_col].astype(int)\n","\n","print(f\"Dataset size: {len(df)}\")\n","print(f\"Viral comments: {df[cfg.label_col].sum()} ({df[cfg.label_col].mean()*100:.2f}%)\")\n","\n","train_df, test_df = train_test_split(\n","    df, test_size=0.15, random_state=cfg.seed, stratify=df[cfg.label_col]\n",")\n","train_df, val_df = train_test_split(\n","    train_df, test_size=0.1765, random_state=cfg.seed, stratify=train_df[cfg.label_col]\n",")\n","\n","print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n","\n","# Split comments into sentences\n","print(\"Splitting comments into sentences\")\n","train_df['sentences'] = train_df[cfg.text_col].apply(split_into_sentences)\n","val_df['sentences'] = val_df[cfg.text_col].apply(split_into_sentences)\n","test_df['sentences'] = test_df[cfg.text_col].apply(split_into_sentences)\n","\n","# Check sentence statistics\n","all_sentence_counts = train_df['sentences'].apply(len)\n","\n","def extract_numeric_features(df):\n","    return np.column_stack([\n","        df[cfg.timestamp_col].values,\n","        df[cfg.day_sin_col].values,\n","        df[cfg.day_cos_col].values,\n","        df[cfg.hour_sin_col].values,\n","        df[cfg.hour_cos_col].values,\n","    ])\n","\n","train_numeric = extract_numeric_features(train_df)\n","val_numeric = extract_numeric_features(val_df)\n","test_numeric = extract_numeric_features(test_df)\n","\n","print(f\"Numeric features shape: {train_numeric.shape}\")\n","print(f\"Numeric features (first row): {train_numeric[0]}\")\n","\n","tokenizer = DistilBertTokenizerFast.from_pretrained(cfg.model_name)\n","\n","class HierarchicalCommentDataset(Dataset):\n","    def __init__(self, sentence_lists, numeric_feats, labels, tokenizer, max_length):\n","        self.sentence_lists = list(sentence_lists)\n","        self.numeric = torch.tensor(numeric_feats, dtype=torch.float32)\n","        self.labels = torch.tensor(np.asarray(labels, dtype=np.float32))\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.sentence_lists)\n","\n","    def __getitem__(self, idx):\n","        sentences = self.sentence_lists[idx]\n","\n","        # Tokenize each sentence separately\n","        sentence_encodings = []\n","        for sent in sentences:\n","            enc = self.tokenizer(\n","                sent,\n","                truncation=True,\n","                max_length=self.max_length,\n","                padding=False,\n","                return_tensors=None,\n","            )\n","            sentence_encodings.append({\n","                \"input_ids\": torch.tensor(enc[\"input_ids\"], dtype=torch.long),\n","                \"attention_mask\": torch.tensor(enc[\"attention_mask\"], dtype=torch.long),\n","            })\n","\n","        return {\n","            \"sentences\": sentence_encodings,\n","            \"num_sentences\": len(sentences),\n","            \"numeric\": self.numeric[idx],\n","            \"labels\": self.labels[idx],\n","        }\n","\n","train_ds = HierarchicalCommentDataset(\n","    train_df['sentences'].values, train_numeric, train_df[cfg.label_col].values,\n","    tokenizer, cfg.max_length\n",")\n","val_ds = HierarchicalCommentDataset(\n","    val_df['sentences'].values, val_numeric, val_df[cfg.label_col].values,\n","    tokenizer, cfg.max_length\n",")\n","test_ds = HierarchicalCommentDataset(\n","    test_df['sentences'].values, test_numeric, test_df[cfg.label_col].values,\n","    tokenizer, cfg.max_length\n",")\n","\n","# Hierarchical Collator\n","base_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","def hierarchical_collate_fn(batch):\n","    \"\"\"Collate sentences from multiple comments into a batch.\"\"\"\n","    max_sentences = max(b[\"num_sentences\"] for b in batch)\n","    batch_size = len(batch)\n","\n","    # Flatten all sentences for batch processing\n","    all_sentences = []\n","    sentence_masks = []  # Track which sentences are real vs padding\n","\n","    for b in batch:\n","        sents = b[\"sentences\"]\n","        num_real = len(sents)\n","\n","        # Add real sentences\n","        all_sentences.extend(sents)\n","        sentence_masks.extend([1] * num_real)\n","\n","        # Pad with dummy sentences\n","        for _ in range(max_sentences - num_real):\n","            all_sentences.append({\n","                \"input_ids\": torch.tensor([tokenizer.cls_token_id, tokenizer.sep_token_id]),\n","                \"attention_mask\": torch.tensor([1, 1]),\n","            })\n","            sentence_masks.append(0)\n","\n","    # Collate all sentences\n","    padded_sentences = base_collator(all_sentences)\n","\n","    # Other data\n","    numerics = torch.stack([b[\"numeric\"] for b in batch])\n","    labels = torch.stack([b[\"labels\"] for b in batch])\n","    sentence_masks = torch.tensor(sentence_masks, dtype=torch.bool)\n","\n","    return {\n","        \"input_ids\": padded_sentences[\"input_ids\"],\n","        \"attention_mask\": padded_sentences[\"attention_mask\"],\n","        \"sentence_masks\": sentence_masks.view(batch_size, max_sentences),\n","        \"max_sentences\": max_sentences,\n","        \"batch_size\": batch_size,\n","        \"numeric\": numerics,\n","        \"labels\": labels,\n","    }\n","\n","train_loader = DataLoader(\n","    train_ds, batch_size=cfg.batch_size, shuffle=True, collate_fn=hierarchical_collate_fn\n",")\n","val_loader = DataLoader(\n","    val_ds, batch_size=cfg.batch_size, shuffle=False, collate_fn=hierarchical_collate_fn\n",")\n","test_loader = DataLoader(\n","    test_ds, batch_size=cfg.batch_size, shuffle=False, collate_fn=hierarchical_collate_fn\n",")\n","\n","# Hierarchical Model\n","class HierarchicalViralClassifier(nn.Module):\n","    def __init__(self, model_name, numeric_in_dim=5, delta_proj_dim=64,\n","                 dropout=0.2, freeze_transformer=False):\n","        super().__init__()\n","        self.transformer = DistilBertModel.from_pretrained(model_name)\n","\n","        if freeze_transformer:\n","            for p in self.transformer.parameters():\n","                p.requires_grad = False\n","            print(\"Transformer weights frozen\")\n","\n","        hidden_size = self.transformer.config.hidden_size  # 768\n","\n","        # Sentence-level attention\n","        self.sentence_attention = nn.Sequential(\n","            nn.Linear(hidden_size, 128),\n","            nn.Tanh(),\n","            nn.Linear(128, 1)\n","        )\n","\n","        # Numeric projection\n","        self.numeric_proj = nn.Sequential(\n","            nn.Linear(numeric_in_dim, delta_proj_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(delta_proj_dim, delta_proj_dim),\n","            nn.GELU(),\n","        )\n","\n","        combined_dim = hidden_size + delta_proj_dim\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(dropout),\n","            nn.Linear(combined_dim, 256),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(256, 1),\n","        )\n","\n","        # Store attention for visualization\n","        self.last_sentence_attention = None\n","        self.last_context_attention = None\n","\n","    def forward(self, input_ids, attention_mask, sentence_masks, max_sentences, batch_size, numeric):\n","        \"\"\"\n","        input_ids: (batch_size * max_sentences, seq_len)\n","        attention_mask: (batch_size * max_sentences, seq_len)\n","        sentence_masks: (batch_size, max_sentences) - True for real sentences\n","        \"\"\"\n","        bert_out = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n","        sentence_embeddings = bert_out.last_hidden_state[:, 0, :]  # CLS tokens\n","\n","        sentence_embeddings = sentence_embeddings.view(batch_size, max_sentences, -1)\n","\n","        attention_scores = self.sentence_attention(sentence_embeddings).squeeze(-1)\n","\n","        # Mask out padding sentences\n","        attention_scores = attention_scores.masked_fill(~sentence_masks, -1e9)\n","\n","        # Softmax to get attention weights\n","        attention_weights = torch.softmax(attention_scores, dim=1)\n","\n","        # Weighted sum of sentence embeddings\n","        comment_embedding = torch.sum(\n","            attention_weights.unsqueeze(-1) * sentence_embeddings,\n","            dim=1\n","        )\n","\n","        num_emb = self.numeric_proj(numeric)\n","\n","        final_embedding = torch.cat([comment_embedding, num_emb], dim=-1)\n","\n","        # Classification\n","        logits = self.classifier(final_embedding).squeeze(-1)\n","\n","        return logits\n","\n","model = HierarchicalViralClassifier(\n","    cfg.model_name,\n","    numeric_in_dim=train_numeric.shape[1],  # Should be 5\n","    delta_proj_dim=cfg.delta_proj_dim,\n","    dropout=cfg.dropout,\n","    freeze_transformer=cfg.freeze_transformer\n",").to(device)\n","total_params, trainable_params = count_parameters(model)\n","\n","train_labels_np = train_df[cfg.label_col].values\n","pos_count = (train_labels_np == 1).sum()\n","neg_count = (train_labels_np == 0).sum()\n","pos_weight = torch.tensor(neg_count / max(pos_count, 1), dtype=torch.float32).to(device)\n","criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n","\n","optimizer = AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n","num_training_steps = len(train_loader) * cfg.epochs\n","num_warmup_steps = int(cfg.warmup_ratio * num_training_steps)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n","\n","def evaluate(loader, model, criterion=None):\n","    model.eval()\n","    all_labels = []\n","    all_probs = []\n","    total_loss = 0.0\n","\n","    with torch.no_grad():\n","        for batch in loader:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            sentence_masks = batch[\"sentence_masks\"].to(device)\n","            max_sentences = batch[\"max_sentences\"]\n","            batch_size = batch[\"batch_size\"]\n","            numeric = batch[\"numeric\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","\n","            logits = model(\n","                input_ids,\n","                attention_mask,\n","                sentence_masks,\n","                max_sentences,\n","                batch_size,\n","                numeric\n","            )\n","            probs = torch.sigmoid(logits)\n","\n","            if criterion is not None:\n","                loss = criterion(logits, labels)\n","                total_loss += loss.item()\n","\n","            all_labels.append(labels.cpu().numpy())\n","            all_probs.append(probs.cpu().numpy())\n","\n","    all_labels = np.concatenate(all_labels)\n","    all_probs = np.concatenate(all_probs)\n","    preds = (all_probs >= 0.5).astype(np.int32)\n","\n","    acc = accuracy_score(all_labels, preds)\n","    f1 = f1_score(all_labels, preds)\n","    pr_auc = average_precision_score(all_labels, all_probs)\n","    try:\n","        roc = roc_auc_score(all_labels, all_probs)\n","    except ValueError:\n","        roc = float(\"nan\")\n","\n","    avg_loss = total_loss / len(loader) if criterion is not None else None\n","    return acc, f1, roc, pr_auc, avg_loss\n","\n","best_val_f1 = -1.0\n","for epoch in range(1, cfg.epochs + 1):\n","    model.train()\n","    total_loss = 0.0\n","    for batch in train_loader:\n","        optimizer.zero_grad()\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        sentence_masks = batch[\"sentence_masks\"].to(device)\n","        max_sentences = batch[\"max_sentences\"]\n","        batch_size = batch[\"batch_size\"]\n","        numeric = batch[\"numeric\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","\n","        logits = model(\n","            input_ids,\n","            attention_mask,\n","            sentence_masks,\n","            max_sentences,\n","            batch_size,\n","            numeric\n","        )\n","        loss = criterion(logits, labels)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        scheduler.step()\n","        total_loss += loss.item()\n","\n","    train_avg_loss = total_loss / len(train_loader)\n","\n","    # Evaluate on train set (for F1)\n","    train_acc, train_f1, train_roc, train_pr_auc, _ = evaluate(train_loader, model)\n","\n","    # Evaluate on validation set\n","    val_acc, val_f1, val_roc, val_pr_auc, val_avg_loss = evaluate(val_loader, model, criterion)\n","\n","    print(f\"Epoch {epoch}/{cfg.epochs}\")\n","    print(f\"  Train Loss: {train_avg_loss:.4f} | Train F1: {train_f1:.4f}\")\n","    print(f\"  Val Loss: {val_avg_loss:.4f} | Val F1: {val_f1:.4f} | Val Acc: {val_acc:.4f} | \"\n","          f\"Val ROC-AUC: {val_roc:.4f} | Val PR-AUC: {val_pr_auc:.4f}\")\n","\n","    if val_f1 > best_val_f1:\n","        best_val_f1 = val_f1\n","        torch.save({\n","            \"model_state_dict\": model.state_dict(),\n","            \"tokenizer_name\": cfg.model_name,\n","            \"config\": vars(cfg),\n","        }, cfg.save_best_path)\n","        print(f\"  ✓ Saved best model to {cfg.save_best_path}\")\n","    print()\n","\n","test_acc, test_f1, test_roc, test_pr_auc, test_loss = evaluate(test_loader, model, criterion)\n","print(f\"\\n{'='*60}\")\n","print(f\"FINAL TEST RESULTS:\")\n","print(f\"{'='*60}\")\n","print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f} | Test F1: {test_f1:.4f} | \"\n","      f\"Test ROC-AUC: {test_roc:.4f} | Test PR-AUC: {test_pr_auc:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":440091,"status":"ok","timestamp":1764974027533,"user":{"displayName":"Sameer Agrawal","userId":"18062353540738892167"},"user_tz":480},"id":"dvrVIFrDfXtn","outputId":"d034b94a-c123-48c8-c534-99f07739f153"},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating hierarchical test dataset...\n","================================================================================\n","DETAILED EVALUATION ON TEST SET (HIERARCHICAL MODEL)\n","================================================================================\n","\n","--------------------------------OVERALL METRICS---------------------------------\n","Samples: 19415 (Viral: 3236, Non-viral: 16179)\n","Threshold: 0.5\n","\n","Accuracy:          0.9075\n","Balanced Accuracy: 0.8664\n","F1 (binary, pos):  0.7437\n","F1 (macro):        0.8436\n","F1 (weighted):     0.9103\n","ROC-AUC:           0.9496\n","PR-AUC:            0.7866\n","\n","-------------------------------PER-CLASS METRICS--------------------------------\n","Class           Precision    Recall       F1-Score     Support   \n","--------------------------------------------------------------------------------\n","Non-viral (0)   0.9596       0.9281       0.9436       16179     \n","Viral (1)       0.6913       0.8047       0.7437       3236      \n","\n","--------------------------------CONFUSION MATRIX--------------------------------\n","                     Predicted Non-viral  Predicted Viral     \n","Actual Non-viral     15016                1163                \n","Actual Viral         632                  2604                \n","\n","--------------------------------DETAILED COUNTS---------------------------------\n","True Negatives (TN):  15016      (correctly predicted non-viral)\n","False Positives (FP): 1163       (predicted viral, actually non-viral)\n","False Negatives (FN): 632        (predicted non-viral, actually viral)\n","True Positives (TP):  2604       (correctly predicted viral)\n","\n","------------------------------PREDICTIONS SUMMARY-------------------------------\n","Predicted as viral:     3767 comments\n","Predicted as non-viral: 15648 comments\n","================================================================================\n","\n","================================================================================\n","THRESHOLD SENSITIVITY ANALYSIS\n","================================================================================\n","Threshold    Precision    Recall       F1           Predicted Viral\n","--------------------------------------------------------------------------------\n","0.3          0.6734       0.8232       0.7408       3956           \n","0.4          0.6838       0.8121       0.7425       3843           \n","0.5          0.6913       0.8047       0.7437       3767           \n","0.6          0.6982       0.7942       0.7431       3681           \n","0.7          0.7088       0.7846       0.7448       3582           \n","================================================================================\n"]}],"source":["from sklearn.metrics import (\n","    accuracy_score,\n","    balanced_accuracy_score,\n","    f1_score,\n","    precision_recall_fscore_support,\n","    confusion_matrix,\n","    roc_auc_score,\n","    average_precision_score,\n",")\n","\n","df = pd.read_csv(cfg.data_csv)\n","df = df.dropna(subset=[cfg.text_col] + ['timestamp_scaled', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos'] + [cfg.label_col])\n","df[cfg.label_col] = df[cfg.label_col].astype(int)\n","\n","train_df, test_df = train_test_split(\n","    df, test_size=0.15, random_state=cfg.seed, stratify=df[cfg.label_col]\n",")\n","\n","test_df['sentences'] = test_df[cfg.text_col].apply(split_into_sentences)\n","\n","test_numeric = test_df[['timestamp_scaled', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos']].values\n","\n","# Create hierarchical dataset\n","test_ds_hierarchical = HierarchicalCommentDataset(\n","    test_df['sentences'].values,\n","    test_numeric,\n","    test_df[cfg.label_col].values,\n","    tokenizer,\n","    cfg.max_length\n",")\n","\n","# Create hierarchical loader\n","test_loader_hierarchical = DataLoader(\n","    test_ds_hierarchical,\n","    batch_size=cfg.batch_size,\n","    shuffle=False,\n","    collate_fn=hierarchical_collate_fn\n",")\n","\n","def evaluate_detailed_hierarchical(loader, model, threshold=0.5):\n","    \"\"\"Evaluation function specifically for hierarchical model.\"\"\"\n","    model.eval()\n","    y_true = []\n","    y_prob = []\n","\n","    with torch.no_grad():\n","        for batch in loader:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            sentence_masks = batch[\"sentence_masks\"].to(device)\n","            max_sentences = batch[\"max_sentences\"]\n","            batch_size = batch[\"batch_size\"]\n","            numeric = batch[\"numeric\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","\n","            # Hierarchical model forward pass\n","            logits = model(\n","                input_ids,\n","                attention_mask,\n","                sentence_masks,\n","                max_sentences,\n","                batch_size,\n","                numeric\n","            )\n","            probs = torch.sigmoid(logits)\n","\n","            y_true.append(labels.cpu().numpy())\n","            y_prob.append(probs.cpu().numpy())\n","\n","    y_true = np.concatenate(y_true).astype(int)\n","    y_prob = np.concatenate(y_prob)\n","    y_pred = (y_prob >= threshold).astype(int)\n","\n","    # Overall metrics\n","    acc = accuracy_score(y_true, y_pred)\n","    bal_acc = balanced_accuracy_score(y_true, y_pred)\n","    f1_binary = f1_score(y_true, y_pred)\n","    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n","        y_true, y_pred, average=\"macro\", zero_division=0\n","    )\n","    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n","        y_true, y_pred, average=\"weighted\", zero_division=0\n","    )\n","\n","    # Per-class metrics\n","    precision_cls, recall_cls, f1_cls, support_cls = precision_recall_fscore_support(\n","        y_true, y_pred, labels=[0, 1], average=None, zero_division=0\n","    )\n","\n","    # Confusion matrix\n","    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n","    tn, fp, fn, tp = cm.ravel()\n","\n","    # ROC-AUC and PR-AUC\n","    roc = None\n","    pr_auc = None\n","    if len(set(y_true)) == 2:\n","        try:\n","            roc = roc_auc_score(y_true, y_prob)\n","        except Exception:\n","            roc = None\n","        try:\n","            pr_auc = average_precision_score(y_true, y_prob)\n","        except Exception:\n","            pr_auc = None\n","\n","    metrics = {\n","        \"threshold\": threshold,\n","        \"n_samples\": int(len(y_true)),\n","        \"positives_true\": int(y_true.sum()),\n","        \"negatives_true\": int((y_true == 0).sum()),\n","        \"positives_pred\": int(y_pred.sum()),\n","        \"negatives_pred\": int((y_pred == 0).sum()),\n","\n","        \"accuracy\": acc,\n","        \"balanced_accuracy\": bal_acc,\n","        \"f1_binary_pos\": f1_binary,\n","        \"precision_macro\": precision_macro,\n","        \"recall_macro\": recall_macro,\n","        \"f1_macro\": f1_macro,\n","        \"precision_weighted\": precision_weighted,\n","        \"recall_weighted\": recall_weighted,\n","        \"f1_weighted\": f1_weighted,\n","\n","        \"class_0\": {\n","            \"precision\": precision_cls[0],\n","            \"recall\": recall_cls[0],\n","            \"f1\": f1_cls[0],\n","            \"support\": int(support_cls[0]),\n","        },\n","        \"class_1\": {\n","            \"precision\": precision_cls[1],\n","            \"recall\": recall_cls[1],\n","            \"f1\": f1_cls[1],\n","            \"support\": int(support_cls[1]),\n","        },\n","\n","        \"confusion_matrix\": cm.tolist(),\n","        \"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp),\n","\n","        \"roc_auc\": roc,\n","        \"pr_auc\": pr_auc,\n","    }\n","\n","    return metrics\n","\n","\n","# -----------------------\n","# Load and evaluate\n","# -----------------------\n","ckpt = torch.load(cfg.save_best_path, map_location=device)\n","conf = ckpt.get(\"config\", {})\n","\n","model_loaded = HierarchicalViralClassifier(\n","    model_name=conf.get(\"model_name\", cfg.model_name),\n","    numeric_in_dim=5,\n","    delta_proj_dim=conf.get(\"delta_proj_dim\", cfg.delta_proj_dim),\n","    dropout=conf.get(\"dropout\", cfg.dropout),\n","    freeze_transformer=conf.get(\"freeze_transformer\", cfg.freeze_transformer),\n",").to(device)\n","\n","model_loaded.load_state_dict(ckpt[\"model_state_dict\"])\n","model_loaded.eval()\n","\n","print(\"=\"*80)\n","print(\"DETAILED EVALUATION ON TEST SET (HIERARCHICAL MODEL)\")\n","print(\"=\"*80)\n","\n","# Evaluate with hierarchical loader\n","metrics = evaluate_detailed_hierarchical(test_loader_hierarchical, model_loaded, threshold=0.5)\n","\n","# Print comprehensive summary\n","print(f\"\\n{'OVERALL METRICS':-^80}\")\n","print(f\"Samples: {metrics['n_samples']} (Viral: {metrics['positives_true']}, \"\n","      f\"Non-viral: {metrics['negatives_true']})\")\n","print(f\"Threshold: {metrics['threshold']}\")\n","print(f\"\\nAccuracy:          {metrics['accuracy']:.4f}\")\n","print(f\"Balanced Accuracy: {metrics['balanced_accuracy']:.4f}\")\n","print(f\"F1 (binary, pos):  {metrics['f1_binary_pos']:.4f}\")\n","print(f\"F1 (macro):        {metrics['f1_macro']:.4f}\")\n","print(f\"F1 (weighted):     {metrics['f1_weighted']:.4f}\")\n","print(f\"ROC-AUC:           {metrics['roc_auc']:.4f}\")\n","print(f\"PR-AUC:            {metrics['pr_auc']:.4f}\")\n","\n","print(f\"\\n{'PER-CLASS METRICS':-^80}\")\n","print(f\"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\n","print(\"-\" * 80)\n","print(f\"{'Non-viral (0)':<15} {metrics['class_0']['precision']:<12.4f} \"\n","      f\"{metrics['class_0']['recall']:<12.4f} {metrics['class_0']['f1']:<12.4f} \"\n","      f\"{metrics['class_0']['support']:<10}\")\n","print(f\"{'Viral (1)':<15} {metrics['class_1']['precision']:<12.4f} \"\n","      f\"{metrics['class_1']['recall']:<12.4f} {metrics['class_1']['f1']:<12.4f} \"\n","      f\"{metrics['class_1']['support']:<10}\")\n","\n","print(f\"\\n{'CONFUSION MATRIX':-^80}\")\n","print(f\"{'':>20} {'Predicted Non-viral':<20} {'Predicted Viral':<20}\")\n","print(f\"{'Actual Non-viral':<20} {metrics['tn']:<20} {metrics['fp']:<20}\")\n","print(f\"{'Actual Viral':<20} {metrics['fn']:<20} {metrics['tp']:<20}\")\n","\n","print(f\"\\n{'DETAILED COUNTS':-^80}\")\n","print(f\"True Negatives (TN):  {metrics['tn']:<10} (correctly predicted non-viral)\")\n","print(f\"False Positives (FP): {metrics['fp']:<10} (predicted viral, actually non-viral)\")\n","print(f\"False Negatives (FN): {metrics['fn']:<10} (predicted non-viral, actually viral)\")\n","print(f\"True Positives (TP):  {metrics['tp']:<10} (correctly predicted viral)\")\n","\n","print(f\"\\n{'PREDICTIONS SUMMARY':-^80}\")\n","print(f\"Predicted as viral:     {metrics['positives_pred']} comments\")\n","print(f\"Predicted as non-viral: {metrics['negatives_pred']} comments\")\n","\n","print(\"=\"*80)\n","\n","# Threshold sensitivity analysis\n","print(\"\\n\" + \"=\"*80)\n","print(\"THRESHOLD SENSITIVITY ANALYSIS\")\n","print(\"=\"*80)\n","print(f\"{'Threshold':<12} {'Precision':<12} {'Recall':<12} {'F1':<12} {'Predicted Viral':<15}\")\n","print(\"-\" * 80)\n","\n","for thresh in [0.3, 0.4, 0.5, 0.6, 0.7]:\n","    m = evaluate_detailed_hierarchical(test_loader_hierarchical, model_loaded, threshold=thresh)\n","    print(f\"{thresh:<12.1f} {m['class_1']['precision']:<12.4f} \"\n","          f\"{m['class_1']['recall']:<12.4f} {m['class_1']['f1']:<12.4f} \"\n","          f\"{m['positives_pred']:<15}\")\n","\n","print(\"=\"*80)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}